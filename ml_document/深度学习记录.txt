神经网络科普系列 https://yq.aliyun.com/articles/96427?spm=a2c4e.11153940.blogcont86580.15.54d07c9cXnzGXb

从卷积计算到卷积神经网络CNN  https://blog.csdn.net/on2way/article/details/50528346

看到的最直白清晰的CNN讲解   https://blog.csdn.net/meyh0x5vDTk48P2/article/details/79050148

先简单给大家介绍一下什么叫做卷积，学过概率论的同学一定不陌生，那么白话解释一下啥叫卷积，很形象，就是“卷”，你假象一下，
你把擦脸毛巾卷起来，成为一个圆柱体的卷，就是这个意思，抽象的说，就是将原来的对象变小一些，但又能保证原来图像中的信息尽量多的保留下来
 根据上文链接中的讲解和识图去理解CNN，从神经网络最初的感知机开始，拿了数字图片识别的例子，
    一个1000X1000像素的图片转换为矩阵，这个矩阵就是神经网络的输入层，有100万个输入层节点，隐藏层同样有100万神经节点，全连接有10万亿
	引入图卷积之后
	原始矩阵-----卷积核----->卷积层，这是一个矩阵特征提取的过程，相当于降维处理，得到浓缩的可以代表原矩阵的10*10的参数矩阵，就是所谓的卷积核！！！
	原始矩阵卷积操作之后就是卷积层，是一层隐藏层
	
	之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用
	
	池化-Down-pooling（下采样）
	深度学习，自然有多个隐藏层，对应到CNN中，就是有多个卷积层，且有多个卷积核，那么原始图像的过大会导致卷积后的结果过大。
	其实我们可以对卷积后的结果进行一个缩小的过程，使下一次卷积更加轻松。这个缩小过程就是池化。
	
	最后整个过程就是：首先有输入层A，初始化一个卷积核，然后进行卷积，得到了第一个卷积层B（如果有i个卷积核，就会在同一层中得到），
	然后进行池化，得到一个池化层C（），然后进行下一次卷积。。。一个卷积层，一个池化层，重复下去，假如是做分类任务，那么当层数到达了我们指定的层数，
	然后到达了输出层，CNN 中的输出层是全连接层，其中来自其他层的输入在这里被平化和发送，以便将输出转换为网络所需的参数。
	通过前向传播过程到达label，然后进行反向传播，进行参数的计算。
	
	再多说一些 
	之前说的多个卷积核，必然得到多个输出，这些输出就是一个个的矩阵，而这个矩阵本身其实也是图片（就像原始图像一样），
	这些图片叫做feature maps，由于feature maps 是由不同卷积和得到的，卷积和是一种特征提取，与输入的图像进行卷积后，
	相当于再做“激活”动作，激活后得到的feature maps 就是具有对应卷积核特征的图片，卷积核其实就是滤波器，
	符合我特征的，激活，不符合的，死着呆着，换做另一个卷积核，可能之前被激活的这次就没有，而之前死的这次被激活了，
	所以feature maps 是被卷积核过滤出来的具有不同特征的图片，最后的输出层就是汇总了这些特征的图片，
	也就是说，到达输出层这里的图片，具有前面每个卷积核的特征，或者可以理解为，到达输出层的图片是输入图片的最明显的特征的集合体，
	就像人类对一个图片做判断，一个小狗在草地上，另一个图片是一个小猫在水里，那么你判断图中的动物是猫还是狗，几乎不会受到草坪或者水面的影响，
	而是基于图片主体本身，或者说基于猫和狗的不同特征，这就是CNN在模仿人类的判断方式。
	
	卷积层从数据中提取有用的特征；激活层为网络中引入非线性，增强网络表征能力；池化层通过采样减少特征维度，
	并保持这些特征具有某种程度上的尺度变化不变性。在全连接层实施对象的分类预测
	
	
	这个讲的很棒 https://yq.aliyun.com/articles/156269?spm=a2c4e.11153940.blogcont86580.19.54d07c9cXnzGXb
	在本质上，离散卷积就是一个线性运算。因此，这样的卷积操作也被称为线性滤波。这里的“线性”是指，我们用每个像素的邻域的线性组合来代替这个像素
	比如某一个点卷积之后就变成了 wa+xb+ye+zf, 这个新的像素点取了之前这个点周围的像素的线性组合，而线性组合的系数就是卷积核。为了得到最佳的系数，也就是卷积核
	网络就是同步不断的训练，通过反向传播算法来修正系数，卷积核就是网络中的权重参数
	
	了解反向传播算法怎么样一步步的通过反向传播算法来找到最佳权值参数的 https://yq.aliyun.com/articles/110025?spm=a2c4e.11153940.blogcont86580.17.54d07c9cXnzGXb
	损失函数L = f(w)- Y  f(w)又可以转化为权值和输入元素的线性组合，输入元素是常量，权重是变量，最后损失函数变成了一个单纯和权值相关的函数，
	而找到最佳权值就是为了找到损失函数的最小或是极小值，找到一个函数的极小值通常是通过梯度下降来逼近极小值，最终确定极小值得位置就是最佳权重
	为了求出这个梯度，需要求出损失函数对每一个权值的偏导数。
	
	因为某一个节点的值是与它相连接的节点和权值的线性组合，这种简单的线性组合比如 z = ax + by  
	我们发现a和b不仅仅是系数 也是对应的偏导因为 z = @z/@x *x + @z/@y *y,而权重在CNN中就是卷积核，也就是说卷积核就是一个个偏导组成的