
线性代数的本质理解   https://www.cnblogs.com/zf-blog/p/8108447.html

在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）
简而言之，在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。是的，矩阵的本质是运动的描述
向量---对象  矩阵---运动(变换)
一旦我们理解了“变换”这个概念，矩阵的定义就变成：矩阵是线性空间里的变换的描述

线性变换
线性变换究竟是一种什么样的变换？我们刚才说了，变换是从空间的一个点跃迁到另一个点，而线性变换，
就是从一个线性空间V的某一个点跃迁到另一个线性空间W的另一个点的运动。这句话里蕴含着一层意思，就是
说一个点不仅可以变换到同一个线性空间中的另一个点，而且可以变换到另一个线性空间中的另一个点去。不管你怎么变，只要变换前后都是线性空间中的对象，这
个变换就一定是线性变换，也就一定可以用一个非奇异矩阵来描述。而你用一个非奇异矩阵去描述的一个变换，一定是一个线性变换。

相似矩阵
若矩阵A与B是同一个线性变换的两个不同的描述（之所以会不同，是因为选定了不同的基，也就是选定了不同的坐标系），则一定能找到一
个非奇异矩阵P，使得A、B之间满足这样的关系：A=P−1BP。线性代数稍微熟一点的读者一下就看出来，这就是相似矩阵的定义。没错，所谓相似矩阵，就是同
一个线性变换的不同的描述矩阵。按照这个定义，同一头猪的不同角度的照片也可以成为相似照片。俗了一点，不过能让人明白。而在上面式子里那个矩阵P，其
实就是A矩阵所基于的基与B矩阵所基于的基这两组基之间的一个变换关系

原来一族相似矩阵都是同一个线性变换的描述啊！难怪这么重要！工科研究生课程中有矩阵论、矩阵分析等课程，其中讲了各种各样的相似变换，比如什么相
似标准型，对角化之类的内容，都要求变换以后得到的那个矩阵与先前的那个矩阵式相似的，为什么这么要求？因为只有这样要求，才能保证变换前后的两个矩阵
是描述同一个线性变换的

当然，同一个线性变换的不同矩阵描述，从实际运算性质来看并不是不分好环的。有些描述矩阵就比其他的矩阵性质好得多。这很容易理解，同一头猪的照片也有
美丑之分嘛。所以矩阵的相似变换可以把一个比较丑的矩阵变成一个比较美的矩阵，而保证这两个矩阵都是描述了同一个线性变换。这样一来，矩阵作为线性变换
描述的一面，基本上说清楚了。但是，事情没有那么简单，或者说，线性代数还有比这更奇妙的性质，那就是，矩阵不仅可以作为线性变换的描述，而且可以作为
一组基的描述。而作为变换的矩阵，不但可以把线性空间中的一个点给变换到另一个点去，而且也能够把线性空间中的一个坐标系（基）表换到另一个坐标系（基）
去。而且，变换点与变换坐标系，具有异曲同工的效果。线性代数里最有趣的奥妙，就蕴含在其中。理解了这些内容，线性代数里很多定理和规则会变得更加清晰
、直觉。

首先来总结一下前面部分的一些主要结论：
1.首先有空间，空间可以容纳对象运动的。一种空间对应一类对象。
2.有一种空间叫线性空间，线性空间是容纳向量对象运动的。
3.运动是瞬时的，因此也被称为变换。
4.矩阵是线性空间中运动（变换）的描述。
5.矩阵与向量相乘，就是实施运动（变换）的过程。
6.同一个变换，在不同的坐标系下表现为不同的矩阵，但是它们的本质是一样的，所以本征值相同。

之所以矩阵又是运动，又是坐标系，那是因为——“运动等价于坐标系变换”。对不起，这话其实不准确，
我只是想让你印象深刻。准确的说法是：“对象的变换等价于坐标系的变换”。或
者：“固定坐标系下一个对象的变换等价于固定对象所处的坐标系变换。”说白了就是：“运动是相对的。” 
 直观的理解可以这样想 在A坐标系，从(1,1)点变换到(1,2) 等价于这个点不变，然后A坐标系向X负方向移动一个单位，
 就是参考不一样而已，前面是点相对于坐标系运动，后面是坐标系相对于点运动，故如果矩阵用来描述运动，变换，那么这时候是对象相对于基运动，
 如果用矩阵来描述基的时候，就是基相对于一个空间固定点（对象）的运动
 
 让我们想想，达成同一个变换的结果，比如把点(1,1)变到点(2,3)去，你可以有两种做法。第一，坐标系不动，点动，把(1,1)点挪到(2,3)去。第二，点不动，变
坐标系，让x轴的度量（单位向量）变成原来的1/2，让y轴的度量（单位向量）变成原先的1/3，这样点还是那个点，可是点的坐标就变成(2,3)了。方式不同，结
果一样。从第一个方式来看，那就是把矩阵看成是运动描述，矩阵与向量相乘就是使向量（点）运动的过程。在这个方式下，Ma=b的意思是：“向量a经过矩阵M
所描述的变换，变成了向量b。”而从第二个方式来看，矩阵M描述了一个坐标系，姑且也称之为M。那么：Ma=b的意思是：“有一个向量，它在坐标系M的度量下
得到的度量结果向量为a，那么它在坐标系I的度量下，这个向量的度量结果是b。”这里的I是指单位矩阵，就是主对角线是1，其他为零的矩阵。而这两个方式本质
上是等价的。我希望你务必理解这一点，因为这是本篇的关键。正因为是关键，所以我得再解释一下。在M为坐标系的意义下，如果把M放在一个向量a的前面，形
成Ma的样式，我们可以认为这是对向量a的一个环境声明。它相当于是说：“注意了！这里有一个向量，它在坐标系M中度量，得到的度量结果可以表达为a。可是
它在别的坐标系里度量的话，就会得到不同的结果。为了明确，我把M放在前面，让你明白，这是该向量在坐标系M中度量的结果。”

Ma=b == Ma=Ib == M-1Mb
对于这个等式去理解矩阵本质  
    1，矩阵是运动是变换：在同一坐标系下面，向量a经过矩阵M所描述的变换变成了向量b
	2  矩阵是坐标系是基： 有一个向量，它在坐标系M的度量下得到的度量结果向量为a，那么它在坐标系I的度量下，这个向量的度量结果是b。I是指单位矩阵

M-1M = I 这就是坐标系的变换，M的逆矩阵这里作为运动，而矩阵M和单位矩阵I这里都可以看做坐标系
  之前只是知道矩阵与向量乘法的意义，这里通过上述可以知道矩阵乘法的本质：
      “对坐标系施加变换的方法，就是让表示那个坐标系的矩阵与表示那个变化的矩阵相乘。”再一次的，矩阵的乘法变成了运动的施加。
	  
	  进一步理解，因为矩阵又是向量组成的向量组
	  .从变换的观点看，对坐标系M施加M-1变换，就是把组成坐标系M的每一个向量施加M-1变换。
	  
	  
	  最终完全可以通过 Ma = b 和M-1M = I 理解矩阵的本质
	  Ma = b :  1 在同一坐标系下，向量a通过线性变换M 变成了向量b，，哪一个坐标系 Ma = b 可以写成 MaI = b 在标准坐标系I下
	            2 如果确定一点不变，这个向量在M坐标系下，可以表示成a，在单位坐标系I下可以表示成b
		
      M-1M = I  ： 1在M坐标系下，通过可以通过线性变换M-1变成另一个坐标系I，本质就是对原坐标系的每一个向量做线性变换M-1，组成了一个新的坐标系I
	  

ok 我们来理解一下相似矩阵 https://spaces.ac.cn/archives/1777  
 相似矩阵是同一线性变换的不同基下的不同描述，比如空间中两个点从一个点到另一个点的移动，我们要想描述这个运动，首先得选取参考系
 在标准坐标系I下描述是 Aa = c 在I坐标系下这个运动描述是矩阵A 但是在坐标系P下去描述变成了 Bb = d ，所以本质上A和B这两个矩阵代表的运动是一样的
 因为我们选择了不同的参考系所以描述不一样而已，我们说A和B矩阵相似，而且A和B之间可以矩阵变换过来，B = P-1AP
 
 这里我们需要明白，其实选定了标准坐标系I之后，A是I下的描述，P这个坐标系也是I下的描述，但是B不是I下的描述，是I作用了P之后下的坐标系下的描述
 那么相似矩阵的充要条件是怎么推导出来的呢，
     Aa = c  (I 坐标系下)
	 
	 Bb = d   (P 坐标系下)
	 
	 b = Pa 且 d = Pc
	 
	 将A和B带入 》 Bb = Pc = PAa ==> BPa = PAa ==> BP = PA ==> B = P-1AP
	 
	 
2 特征工程
  特征值与特征向量的理解 https://www.zhihu.com/question/21874816
  
  向量的内积的意义 a*b = abcos@   内积是一个数字
  向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积
  a·b>0    方向基本相同，夹角在0°到90°之间
  a·b=0    正交，相互垂直  
  a·b<0    方向基本相反，夹角在90°到180°之间 
  
  矩阵的构成可以看做向量组，多个向量组成的矩阵，矩阵与向量的相乘， A22 * a21, 就是每一行向量分别于这个向量的内积

---------------------
    Av = lambda*v  把一个矩阵作用在一个向量上的效果和把一个常数作用在这个向量上的效果一样，\lambda就是特征值，
   从特征向量和特征值的定义式还可以看出，特征向量所在直线上的向量都是特征向量
   
   要观察矩阵所代表的运动，需要把它附加到向量上才观察的出来：
   
3 协方差矩阵的应用与理解 
  协方差矩阵的计算  https://blog.csdn.net/sh199210/article/details/51778028/
  
  1 单个向量的性质特性
     EX 向量的期望：主要是表达这个向量的平均值
	 DX 向量的方差：主要表达的是这个向量的的每一个变量与其均值的偏离程度， 各个变量（X-EX）的和的平方
	 [8,9,10],[2,9,16] 两个向量的期望一样，方差却偏差很大
  2 两个向量之间的关系 二维数据
     cov(x,y) 协方差： 表示Xy之间相互关系的数字特征 cov(x,y) = (x-Ex)(y-Ey)
	 当Cov(X, Y) > 0时，X与Y正相关；
     当Cov(X, Y) < 0时，X与Y负相关；
     当Cov(X, Y) = 0时，X与Y不相关，独立；
	 
	 p(x,y) 除以两变量标准差之积以标准化，即相关系数 皮尔逊相关系数
	 
  3 多个向量之间的特性
     协方差矩阵： 就是多个变量两两间协方差值，按顺序排成的矩阵
	 比如二维矩阵的协方差矩阵的计算过程就是
	 [ cov(x,x),cov(x,y) ]
	 [ cov(y,x),cov(y,y) ] 所以说协方差矩阵是一个对称矩阵，主对角线就是c(x,x) = (x-Ex)(x-Ex) = DX
	 
  4 协方差矩阵与原矩阵的关系
    # 对于机器学习领域的PCA来说，如果遇到的矩阵不是方阵，需要计算他的协方差矩阵来进行下一步计算，
    # 因为协方差矩阵一定是方阵，而特征值分解针对的必须是方阵，svd针对的可以是非方阵情况。
    # 协方差矩阵其实保留了原矩阵的主要数据特性，每一个向量本身的性质DX，以及各个向量之间的性质cov（x, y）
    # 也就是说，每一个向量的数据之间的离散程度我们保留了，而且各个向量之间的相关性我们也保留下来了，接下来可以去进行特征值分解了
     












